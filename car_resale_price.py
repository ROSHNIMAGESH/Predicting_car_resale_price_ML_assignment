# -*- coding: utf-8 -*-
"""Car_Resale_Price.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X2S_bnu9ySPrtzGqajAa-ZZgGo2iLC29
"""

#importing pandas library and assign an alias pd to the pandas
import pandas as pd

#upload files from local machine into the Colab environment
from google.colab import files
uploaded=files.upload()

#read a CSV (Comma-Separated Values) file into a Pandas framework
df = pd.read_csv('car_resale_price_prediction_dataset.csv')

#overview of the DataFrame
df.info()
#1.no of rows
#2.no of columns
#3.column name - non-null values count - datatypes
#4.memory usage of the data frame

#display few rows
df.head()
#df.head() -> displays 1st 5 rows
#df.head(n) -> diaplays 1st n rows

#identify and count the number of missing values
df.isnull().sum()
#identify null values -> df.isnull()

#convert categorical columns into one-hot encoded columns
df = pd.get_dummies(df, columns=['Make', 'Model', 'FuelType', 'Transmission', 'Color', 'CarCondition', 'Location'], drop_first=True)
#drop_first=True -> to avoid multicollinearity (dropping the first category - still retain all the information)

#detecting and removing outliers using IQR(Inter-Quartaile-Range) method
def remove_outliers(df, columns):
  for column in columns:
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    df = df[~((df[column] < (Q1 - 1.5 * IQR)) | (df[column] > (Q3 + 1.5 * IQR)))]
  return df
num_columns = ['Mileage', 'EngineSize']
df_cleaned = remove_outliers(df, num_columns)
df.head()

#defining continuous columns
continuous_columns = ['Mileage', 'EngineSize']
#feature scaling on continuous variables
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[continuous_columns] = scaler.fit_transform(df[continuous_columns])

#create a new feature
import datetime
current_year = datetime.datetime.now().year
df['CarAge'] = current_year - df['Year']
df.drop('Year', axis=1, inplace=True)

#define X (features) and y (target) variable
from sklearn.model_selection import train_test_split
X = df.drop('ResalePrice', axis=1)
y = df['ResalePrice']

#split the data into training and testing sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize and train the model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Make predictions
y_pred_lr = lr_model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred_lr)
mse = mean_squared_error(y_test, y_pred_lr)
r2 = r2_score(y_test, y_pred_lr)
print(f"Linear Regression - MAE: {mae}, MSE: {mse}, R²: {r2}")

from sklearn.ensemble import RandomForestRegressor

# Initialize and train the model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest - MAE: {mae_rf}, MSE: {mse_rf}, R²: {r2_rf}")

from sklearn.ensemble import GradientBoostingRegressor

# Initialize and train the model
gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train, y_train)

# Make predictions
y_pred_gb = gb_model.predict(X_test)

# Evaluate the model
mae_gb = mean_absolute_error(y_test, y_pred_gb)
mse_gb = mean_squared_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)
print(f"Gradient Boosting - MAE: {mae_gb}, MSE: {mse_gb}, R²: {r2_gb}")

from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize Grid Search
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best parameters
grid_search.best_params_

import matplotlib.pyplot as plt

# Remove 'carID' from the features DataFrame
X_filtered = X.drop(columns=['CarID'])

# Plot feature importance for Random Forest
feature_importances = rf_model.feature_importances_

# Remove the feature importance corresponding to 'carID'
carID_index = X.columns.get_loc('CarID')
filtered_importances = [importance for i, importance in enumerate(feature_importances) if i != carID_index]

# Update features list
features = X_filtered.columns

# Ensure that both 'features' and 'filtered_importances' have the same length
plt.figure(figsize=(10, 6))
plt.barh(features, filtered_importances)
plt.xlabel("Feature Importance")
plt.title("Feature Importance for Random Forest Model")
plt.show()

# Make predictions
y_pred = rf_model.predict(X_test)

import numpy as np
# Model Evaluation
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Model Evaluation for Random Forest:")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R²): {r2}")

# Plot Actual vs Predicted
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Resale Prices')
plt.ylabel('Predicted Resale Prices')
plt.title('Actual vs Predicted Resale Prices - Random Forest')
plt.show()

